
> implementations:
  - https://github.com/toji/gl-matrix/blob/master/src/mat4.js#L1539 (perspectiveNO)


# Projections

https://en.wikipedia.org/wiki/3D_projection


## Motivation

ref:
  - [OGLDEV] OpenGL for beginners: https://www.youtube.com/watch?v=h2cP6sQYdf0&list=PLA0dXqQjCx0S04ntJKUftl6OaOgsiwHjA
    - [OGLDEV-Perspective] https://www.youtube.com/watch?v=LhQ85bPCAJ8&list=PLA0dXqQjCx0S04ntJKUftl6OaOgsiwHjA
  - Graphics in 5 min – Perspective Projection https://www.youtube.com/watch?v=F5WA26W4JaM

goal:
  > we have a 3D scene/world that we want to "render" to a 2D screen
  > mathematically, this is *projection* of a 3D space to a 2D space
    > we will call this 2D space the *projection plane* (an infinite plane positioned/oriented in the 3D world)
    > the *viewport* is a bound centered at the origin of the projection plane with width `vw` and height `vh`
    > the *screen* is the area bounded by the viewport
    > the screen is rasterized as pixel data and displayed on some physical *display device*
  > there are several ways to perform projections:
    - orthographic projection: simply discard one dimension (the "depth" dimension orthogonal to the plane)
    - perspective projection: project towards a single focal point (to simulate a virtual camera/eye)

real-world physics:
  > ref: Graphics in 5 min – Perspective Projection [https://www.youtube.com/watch?v=F5WA26W4JaM]
  > we want to simulate how the human eye (or the artificial version, the *camera*) perceives the world
  > a naive "camera" would just be a screen that we expose to light
    > this does not work, because light radiates diffusely in all directions (it would be an utmost blurry "image")
    > e.g. if we have a point on our target object, the light from it will end up on all points on the screen
  > instead, we introduce a small hole (an *aperture*) that only captures light from one specific angle for any source
    > this is a pinhole camera [w/Pinhole_camera]
    > we will consider the idealized scenario where the aperture is a point
      > in real life, there will be some aperture size and this will determine how much light enters
        > the smaller the aperture, the less light, but the clearer (less blurry) the image
        > (in real life, a lens can be introduced to gather more light before it gets focused into the aperture)
  > a *screen* (or sensor) is placed behind the aperture, and this captures (an inverted) image
    > in computer graphics, we often place the screen in front of the "aperture" (virtual camera) instead
    > this achieves the same thing, without the inversion (but doesn't make sense in real life)
  > the sensor size + distance from aperture determines the *field of view*
    > the field of view is an angle determining the range of what we can see [w/Field_of_view] [youtu.be/pUuAx_zFnEk]
  > the field of view determines size of the "cone of vision" (for eyes) or pyramid (for rectangular sensors or screens)


## Virtual camera systems

in computer graphics, when simulating a "virtual" camera: [w/Virtual_camera_system]
  - the "aperture" is a point in space that we call the virtual camera, or just *camera* for short
  - the screen is in front rather than behind the aperture (virtual camera)
  - we always use a pyramid rather than a cone (since we have a rectangular screen)
  - we configure the FOV and the screen size grows/shrinks accordingly (respecting aspect ratio of the viewport)
  - rather than having an infinite pyramid, we *clip* the pyramid using parallel *clip planes*, one near and one far
    > this generates a shape called a *frustum* [w/Frustum], we call this the *view frustum* [w/Viewing_frustum]
    > the *screen* also serves as the near clip plane
    > for the near clip plane, this makes sense because:
      - we don't need to render things that close
      - it avoids division by zero (perspective divide for z=0)
      - since the screen is in front of the "aperture", it makes sense not to render things between screen and camera
    > for the far clip plane, this is done to limit the maximum distance of rendering (which can help performance)
  - we will then want to *project* the view frustum (3D volume) onto the screen (2D plane)

types of projection:
  - perspective projection (discussed above)
  - orthographic projection
    > unlike in real life, we are not restricted by physics of light
    > thus, rather than having the camera be a single point, we can also project light to a screen in parallel
    > alternatively, could think of this as though the focal point is infinitely far away


## Orthographic projection

https://en.wikipedia.org/wiki/Parallel_projection
https://en.wikipedia.org/wiki/Orthographic_projection

orthographic projection matrix:
  > the orthographic projection matrix is very simply, we just "discard" one axis
  > for example, if the z-axis is the depth axis, then the matrix just scales z to 0:
    ⎡1 0 0 0⎤
    ⎢0 1 0 0⎥
    ⎢0 0 0 0⎥
    ⎣0 0 0 1⎦

orthographic camera system:
  > often, we will still have a virtual camera (with view frustum etc.), just with an orthographic projection
  > in this case, we need to do some more work to map from camera space to clip space
  > scenario:
    - we are in camera space (i.e. we have already mapped from world space to camera space):
      - camera is at the origin
      - positive z-axis extends "away" from the camera in the viewing direction, orthogonal to the screen
    - we have some known parameters:
      - viewport aspect ratio (width divided by height) `r`
      - vertical field of view angle `fov` in radians (the horizontal can be derived from the viewport aspect ratio)
      - distance along the z-axis to the origin of the view frustum (clip space) `d`
  > we will first need to translate to clip space origin:
    T = ⎡1 0 0 -(left+right)/2⎤ (`(left+right)/2` is the horizontal midpoint of the frustum)
        ⎢0 1 0 -(top+bottom)/2⎥
        ⎢0 0 1 -(far+near)/2  ⎥
        ⎣0 0 0 1              ⎦
  > and then scale to get a normalized (-1, 1) cube:
    S = ⎡2/w  0   0   0⎤ (`w` here is the frustum width, i.e. `right - left`, and `2/w` is `1/(w/2)`)
        ⎢ 0  2/h  0   0⎥ (i.e. we scale by the inverse half the width/height/depth)
        ⎢ 0   0   2/d 0⎥
        ⎣ 0   0   0   1⎦
  > putting it together, we get the transform P = ST (see [w/Orthographic_projection] for the details)
  > note: we may also want to compensate for left-handed vs right-handed coordinate systems here
    > OpenGL clip space is a *left-handed coordinate system*, meaning the positive z-axis points away from the viewer
      > see: https://stackoverflow.com/questions/4124041/is-opengl-coordinate-system-left-handed-or-right-handed
    > usually however in math and world/local space, we want a right-handed coordinate system (+z towards the viewer)
    > ([w/Orthographic_projection] includes a -1 scaling factor for z here in the translation, for this reason)


## Perspective projection

https://en.wikipedia.org/wiki/Pinhole_camera_model
https://en.wikipedia.org/wiki/Camera_matrix
https://en.wikipedia.org/wiki/3D_projection#Mathematical_formula

https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/opengl-perspective-projection-matrix.html

resources:
  - Graphics in 5 min – Perspective Projections Part 2 https://www.youtube.com/watch?v=g7Pb8mrwcJ0

perspective projection:
  > ref: [g5min] – Perspective Projections Part 2 https://www.youtube.com/watch?v=g7Pb8mrwcJ0
  > given a pinhole camera setup, with: [w/Pinhole_camera_model]
    - left-handed coordinate system
    - an aperture at the origin, with the +z-axis (viewing direction or "optical axis") extending away from the camera
    - an image plane (screen) behind the aperture parallel to x/y ("principal plane") at distance `f` (focal length)
  > then, if we have a point `p` (px, py, pz) somewhere in the world, we can draw a *projection line* through the origin
    and onto the screen, landing on some point `q` (qx, qy) on the screen (we can ignore the z component of `q`)
  > visually: (here, looking along the x axis such that +x is towards the viewer)
       p = (px,py,pz)                      p = (px,py,pz)
      │ ╲                                 │ ╲ q = (qx, qy)
   py │   ╲                            py │qy│╲
      │     ╲    f               ~        │  │f ╲
      <────── • ──── Z                    <────── • Z
         pz     ╲  │ qy                      pz
                  ╲│ q = (-qx, -qy)
    > on the right, we reflected the screen to the positive side of the z-axis, this helps remove the negative signs
    > (also known as a "virtual image plane")
  > this forms two similar triangles, with `qy` the unknown variable we want to determine
    > by similar triangles, we know that qy/f = py/pz, thus qy = f * py/pz = (f/pz) * py
  > doing it for the other component also, we get: q = (f/pz) * p (where `f/pz` is a scalar, and `p`/`q` vectors)
  > in other words, any vector can be projected by multiplying it by the scalar `f/z`, (`z` is the z-comp of the vector)
    > often, `f` will be 1, in which case this is simply dividing by `z`
