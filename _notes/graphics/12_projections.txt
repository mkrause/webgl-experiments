
> implementations:
  - https://github.com/toji/gl-matrix/blob/master/src/mat4.js#L1539 (perspectiveNO)


# Projections

https://en.wikipedia.org/wiki/3D_projection


## Motivation

ref:
  - [OGLDEV] OpenGL for beginners: https://www.youtube.com/watch?v=h2cP6sQYdf0&list=PLA0dXqQjCx0S04ntJKUftl6OaOgsiwHjA
    - [OGLDEV-Perspective] https://www.youtube.com/watch?v=LhQ85bPCAJ8&list=PLA0dXqQjCx0S04ntJKUftl6OaOgsiwHjA
  - Graphics in 5 min – Perspective Projection https://www.youtube.com/watch?v=F5WA26W4JaM

goal:
  > we have a 3D scene/world that we want to "render" to a 2D screen
  > mathematically, this is *projection* of a 3D space to a 2D space
    > we will call this 2D space the *projection plane* (an infinite plane positioned/oriented in the 3D world)
    > the *viewport* is a bound centered at the origin of the projection plane with width `vw` and height `vh`
    > the *screen* is the area bounded by the viewport
    > the screen is rasterized as pixel data and displayed on some physical *display device*
  > there are several ways to perform projections:
    - orthographic projection: simply discard one dimension (the "depth" dimension orthogonal to the plane)
    - perspective projection: project towards a single focal point (to simulate a virtual camera/eye)

real-world physics:
  > ref: Graphics in 5 min – Perspective Projection [https://www.youtube.com/watch?v=F5WA26W4JaM]
  > we want to simulate how the human eye (or the artificial version, the *camera*) perceives the world
  > a naive "camera" would just be a screen that we expose to light
    > this does not work, because light radiates diffusely in all directions (it would be an utmost blurry "image")
    > e.g. if we have a point on our target object, the light from it will end up on all points on the screen
  > instead, we introduce a small hole (an *aperture*) that only captures light from one specific angle for any source
    > this is a pinhole camera [w/Pinhole_camera]
    > we will consider the idealized scenario where the aperture is a point
      > in real life, there will be some aperture size and this will determine how much light enters
        > the smaller the aperture, the less light, but the clearer (less blurry) the image
        > (in real life, a lens can be introduced to gather more light before it gets focused into the aperture)
  > a *screen* (or sensor) is placed behind the aperture, and this captures (an inverted) image
    > in computer graphics, we often place the screen in front of the "aperture" (virtual camera) instead
    > this achieves the same thing, without the inversion (but doesn't make sense in real life)
  > the sensor size + distance from aperture determines the *field of view*
    > the field of view is an angle determining the range of what we can see [w/Field_of_view] [youtu.be/pUuAx_zFnEk]
  > the field of view determines size of the "cone of vision" (for eyes) or pyramid (for rectangular sensors or screens)


## Virtual camera systems

in computer graphics, when simulating a "virtual" camera: [w/Virtual_camera_system]
  - the "aperture" is a point in space that we call the virtual camera, or just *camera* for short
  - the screen is in front rather than behind the aperture (virtual camera)
  - we always use a pyramid rather than a cone (since we have a rectangular screen)
  - we configure the FOV and the screen size grows/shrinks accordingly (respecting aspect ratio of the viewport)
  - rather than having an infinite pyramid, we *clip* the pyramid using parallel *clip planes*, one near and one far
    > this generates a shape called a *frustum* [w/Frustum], we call this the *view frustum* [w/Viewing_frustum]
    > the *screen* also serves as the near clip plane
    > for the near clip plane, this makes sense because:
      - we don't need to render things that close
      - it avoids division by zero (perspective divide for z=0)
      - since the screen is in front of the "aperture", it makes sense not to render things between screen and camera
    > for the far clip plane, this is done to limit the maximum distance of rendering (which can help performance)
  - we will then want to *project* the view frustum (3D volume) onto the screen (2D plane)

types of projection:
  - perspective projection (discussed above)
  - orthographic projection
    > unlike in real life, we are not restricted by physics of light
    > thus, rather than having the camera be a single point, we can also project light to a screen in parallel
    > alternatively, could think of this as though the focal point is infinitely far away


## Orthographic projection

https://en.wikipedia.org/wiki/Parallel_projection
https://en.wikipedia.org/wiki/Orthographic_projection

orthographic projection matrix:
  > the orthographic projection matrix is very simply, we just "discard" one axis
  > for example, if the z-axis is the depth axis, then the matrix just scales z to 0:
    ⎡1 0 0 0⎤
    ⎢0 1 0 0⎥
    ⎢0 0 0 0⎥
    ⎣0 0 0 1⎦

orthographic camera system:
  > often, we will still have a virtual camera (with view frustum etc.), just with an orthographic projection
  > in this case, we need to do some more work to map from camera space to clip space
  > scenario:
    - we are in camera space (i.e. we have already mapped from world space to camera space):
      - camera is at the origin
      - positive z-axis extends "away" from the camera in the viewing direction, orthogonal to the screen
    - we have some known parameters:
      - viewport aspect ratio (width divided by height) `r`
      - vertical field of view angle `fov` in radians (the horizontal can be derived from the viewport aspect ratio)
      - distance along the z-axis to the origin of the view frustum (clip space) `d`
  > we will first need to translate to clip space origin:
    T = ⎡1 0 0 -(left+right)/2⎤ (`(left+right)/2` is the horizontal midpoint of the frustum)
        ⎢0 1 0 -(top+bottom)/2⎥
        ⎢0 0 1 -(far+near)/2  ⎥
        ⎣0 0 0 1              ⎦
  > and then scale to get a normalized (-1, 1) cube:
    S = ⎡2/w  0   0   0⎤ (`w` here is the frustum width, i.e. `right - left`, and `2/w` is `1/(w/2)`)
        ⎢ 0  2/h  0   0⎥ (i.e. we scale by the inverse half the width/height/depth)
        ⎢ 0   0   2/d 0⎥
        ⎣ 0   0   0   1⎦
  > putting it together, we get the transform P = ST (see [w/Orthographic_projection] for the details)
  > note: we may also want to compensate for right-handed vs left-handed coordinate systems here
    > OpenGL clip space is a *left-handed coordinate system*, meaning the positive z-axis points away from the viewer
      > see: https://stackoverflow.com/questions/4124041/is-opengl-coordinate-system-left-handed-or-right-handed
    > usually however in math and world/local space, we want a right-handed coordinate system (+z towards the viewer)
    > ([w/Orthographic_projection] includes a -1 scaling factor for z here in the translation, for this reason)


## Perspective projection

https://en.wikipedia.org/wiki/Pinhole_camera_model
https://en.wikipedia.org/wiki/Camera_matrix
https://en.wikipedia.org/wiki/3D_projection#Mathematical_formula

- https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/opengl-perspective-projection-matrix.html
- pikuma – https://www.youtube.com/watch?v=EqNcqBdrNyI

resources:
  - Graphics in 5 min – Perspective Projections Part 2 https://www.youtube.com/watch?v=g7Pb8mrwcJ0

perspective projection:
  > ref: [g5min] – Perspective Projections Part 2 https://www.youtube.com/watch?v=g7Pb8mrwcJ0
  > given a pinhole camera setup, with: [w/Pinhole_camera_model]
    - right-handed coordinate system (z-axis pointing towards the viewer)
    - an aperture at the origin, with the +z-axis (viewing direction or "optical axis") extending away from the camera
    - an image plane (screen) behind the aperture parallel to x/y ("principal plane") at distance `f` (focal length)
  > then, if we have a point `p` (px, py, pz) somewhere in the world, we can draw a *projection line* through the origin
    and onto the screen, landing on some point `q` (qx, qy) on the screen (we can ignore the z component of `q`)
  > visually: (here, looking along the x axis such that +x is towards the viewer)
       p = (px,py,pz)                      p = (px,py,pz)
      │ ╲     Y                           │ ╲ q = (qx, qy)
   py │   ╲   ^                        py │qy│╲   Y
      │     ╲ │  f               ~        │  │f ╲ ^
      ─────── • ───> Z                    ─────── • ───> Z
         pz     ╲  │ qy                      pz
                  ╲│ q = (-qx, -qy)
    > on the right, we reflected the screen to the positive side of the z-axis, this helps remove the negative signs
    > (also known as a "virtual image plane")
  > this forms two similar triangles, with `qy` the unknown variable we want to determine
    > by similar triangles, we know that qy/f = py/pz, thus qy = f * py/pz = (f/pz) * py
  > doing it for the other component also, we get: q = (f/pz) * p (where `f/pz` is a scalar, and `p`/`q` vectors)
  > in other words, any vector can be projected by multiplying it by the scalar `f/z`, (`z` is the z-comp of the vector)
  > since `pz` will be negative in our case, we actually need to multiply by `f/-z`

perspective projection matrix:
  > parameters: `fov` (vertical FoV in radians), `aspect` (width/height), `zNear`, `zFar`
  > we can derive:
    - `f = zNear` (focal length)
    - `theta = fov / 2` (angle of viewing along the (+Y, -Z)-plane)
    - `viewY = tan(theta) * zNear` (world-space Y-coord of the viewport, i.e. half the screen height)
    - `viewX = viewY * aspect` (derive the X-coord through the aspect ratio)
  > first, we want to project (X, Y) to the image plane: (these will still be in world space coordinates)
    > to do so, we will scale both by `-f/z` (where `f` = `zNear`)
    > for `f` we can just use a scaling matrix, but for `z` (dynamic) we need to use `w` coordinate division
      P = ⎡f 0  0 0⎤ (scale X by the focal length)
          ⎢0 f  0 0⎥ (scale Y by the focal length)
          ⎢0 0  1 0⎥
          ⎣0 0 -1 0⎦ (sets up the perspective divide: w = z, so that we end up dividing by the z component)
  > next, we want to normalize (X, Y) so that the range (-1, -1) to (1, 1) corresponds to the frustum dimensions
    Nxy = scaling(1/viewX, 1/viewY, 1)   (scaling matrix with the given diagonal)
    > note: if we work out the composition `NxyP`, then:
      - Y scaling: `(1 / viewY) * zNear` = `(1 / tan(theta) * zNear) * zNear` = `1 / tan(theta)`
      - X scaling: `(1 / viewX) * zNear` = `(1 / tan(theta) * zNear * aspect) * zNear` = `1 / (tan(theta) * aspect)`
      > the `zNear` cancels out
      > factoring out `s` = `1 / tan(theta)`, we then get a scaling of (s / aspect, s)
      > (in existing implementations, this factor `s` is often called `f`, but this is confusing with the focal length)
  > finally, we need to transform the Z coordinate
    > unlike in actual photography, in computer graphics we need to know the Z coordinate as well (for depth testing)
    > thus instead of an image *plane*, we really have an image *volume*, corresponding to our frustum
    > note that earlier, in the perspective divide, we also divided our z component by `w` (which is the original `z`)
      > i.e. given `z` our input vector z component, we first transform `z` to `z'`, and then the result is `z'/z`
      > we need to take this into account in our transformation (cannot just do what we did for the orthographic case)
    > we normalize Z to fit within the frustum bounds (i.e. between the near/far clip planes) [...TODO...]

projection transformation matrix:
    ⎡s/aspect 0                     0                                    0⎤
    ⎢       0 s                     0                                    0⎥
    ⎢       0 0 farZ / (farZ - nearZ) -1 * (farZ * nearZ) / (farZ - nearZ)⎥
    ⎣       0 0                    -1                                    0⎦
    where s = 1 / Math.tan(fov / 2)
  > z combined formula:
    farZ / (farZ - nearZ) * z - (farZ * nearZ) / (farZ - nearZ)


## Comparison with other projection matrices

- pikuma: https://youtu.be/EqNcqBdrNyI?t=1242
  > same as our result (and in fact ours was based on this one), although:
    - he uses h/w for aspect ratio rather than w/h
    - he uses 1 for `w` in the fourth row, whereas it seems we need -1 for OpenGL
      > from the description:
        "The matrix I'll derive is as used by left-handed coordinate systems like DirectX (OpenGL uses a right-handed
        system)."


- the gl-matrix-like implementations:
  > based on vertical fov and aspect ratio
  - https://gamedev.stackexchange.com/questions/120338/what-does-a-perspective-projection-matrix-look-like-in-opengl
  - `gl-matrix` `perspectiveNO` [https://github.com/toji/gl-matrix/blob/master/src/mat4.js#L1539]
    > matrix:
      ⎡f/aspect 0                 0                   0⎤
      ⎢       0 f                 0                   0⎥
      ⎢       0 0 (far + near) * nf 2 * far * near * nf⎥
      ⎣       0 0                -1                   0⎦
      where f = 1.0 / Math.tan(fovy / 2); nf = 1 / (near - far)
    > `(far + near) * nf` = `(near + far) / (near - far)`         (!= `farZ / (farZ - nearZ)`)
    > `2 * far * near * nf` = `(2 * far * near) / (near - far)`   (!= `-1 * (farZ * nearZ) / (farZ - nearZ)`)
    > combined: `((near + far) / (near - far)) * z + (2 * far * near) / (near - far)`
    > based on some quick graphing between the two, the z formula here is different from ours
      > however, plugging the above matrix into our code doesn't seem to work (produces z values outide [-1, 1] range)
  - twgl `perspective` [https://github.com/greggman/twgl.js/blob/master/src/m4.js#L509]
    > also used by https://webgl2fundamentals.org (same author)
    > same as the gl-matrix version (with slight rewriting)

- OpenGL `GL_PROJECTION` matrix:
  > resources:
    - https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/opengl-perspective-projection-matrix.html
    - http://www.songho.ca/opengl/gl_projectionmatrix.html

others:
  - http://www.opengl-tutorial.org/beginners-tutorials/tutorial-3-matrices/#the-projection-matrix
